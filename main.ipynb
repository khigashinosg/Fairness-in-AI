{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Setup Code (from Spec Sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyhi2018/Desktop/Imperial/Ethics, Fairness, and Explainability in AI/CW1 - Fairness/Fairness-in-AI/venv/lib/python3.11/site-packages/aif360/datasets/standard_dataset.py:143: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[pos, label_name] = favorable_label\n"
     ]
    }
   ],
   "source": [
    "from folktables import folktables\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "\n",
    "# (Age) must be greater than 16 and less than 90,\n",
    "# and (Person weight) must be greater than or equal to 1\n",
    "def employment_filter(data):\n",
    "    \"\"\"Filters for the employment prediction task\"\"\"\n",
    "    df = data\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['AGEP'] < 90]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    return df\n",
    "\n",
    "ACSEmployment = folktables.BasicProblem(\n",
    "    features=[\n",
    "        'AGEP',  # age; for range of values of features please check Appendix B.4 of Retiring Adult: New Datasets for Fair Machine Learning NeurIPS 2021 paper\n",
    "        'SCHL',  # educational attainment\n",
    "        'MAR',   # marital status\n",
    "        'RELP',  # relationship\n",
    "        'DIS',   # disability recode\n",
    "        'ESP',   # employment status of parents\n",
    "        'CIT',   # citizenship status\n",
    "        'MIG',   # mobility status (lived here 1 year ago)\n",
    "        'MIL',   # military service\n",
    "        'ANC',   # ancestry recode\n",
    "        'NATIVITY',  # nativity\n",
    "        'DEAR',   # hearing difficulty\n",
    "        'DEYE',   # vision difficulty\n",
    "        'DREM',   # cognitive difficulty\n",
    "        'SEX',    # sex\n",
    "        'RAC1P',  # recoded detailed race code\n",
    "        'GCL',    # grandparents living with grandchildren\n",
    "    ],\n",
    "    target='ESR',  # employment status recode\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='DIS',\n",
    "    preprocess=employment_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"FL\"], download=True)  # data for Florida state\n",
    "\n",
    "features, label, group = ACSEmployment.df_to_numpy(acs_data)\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(features, columns=ACSEmployment.features)\n",
    "data['label'] = label\n",
    "\n",
    "favorable_classes = [True]\n",
    "protected_attribute_names = [ACSEmployment.group]\n",
    "privileged_classes = np.array([[1]])\n",
    "\n",
    "data_for_aif = StandardDataset(\n",
    "    data,\n",
    "    label_name='label',\n",
    "    favorable_classes=favorable_classes,\n",
    "    protected_attribute_names=protected_attribute_names,\n",
    "    privileged_classes=privileged_classes\n",
    ")\n",
    "\n",
    "privileged_groups = [{'DIS': 1}]\n",
    "unprivileged_groups = [{'DIS': 2}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with C = 1e-08 and solver = newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/n0qzq7z56gj9ts7dnzkhpbr80000gn/T/ipykernel_7995/1559154759.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with C = 1e-08 and solver = lbfgs\n",
      "Training model with C = 1e-08 and solver = liblinear\n",
      "Training model with C = 1e-08 and solver = sag\n",
      "Training model with C = 1e-08 and solver = saga\n",
      "Training model with C = 1e-07 and solver = newton-cg\n",
      "Training model with C = 1e-07 and solver = lbfgs\n",
      "Training model with C = 1e-07 and solver = liblinear\n",
      "Training model with C = 1e-07 and solver = sag\n",
      "Training model with C = 1e-07 and solver = saga\n",
      "Training model with C = 1e-06 and solver = newton-cg\n",
      "Training model with C = 1e-06 and solver = lbfgs\n",
      "Training model with C = 1e-06 and solver = liblinear\n",
      "Training model with C = 1e-06 and solver = sag\n",
      "Training model with C = 1e-06 and solver = saga\n",
      "Training model with C = 1e-05 and solver = newton-cg\n",
      "Training model with C = 1e-05 and solver = lbfgs\n",
      "Training model with C = 1e-05 and solver = liblinear\n",
      "Training model with C = 1e-05 and solver = sag\n",
      "Training model with C = 1e-05 and solver = saga\n",
      "Training model with C = 0.0001 and solver = newton-cg\n",
      "Training model with C = 0.0001 and solver = lbfgs\n",
      "Training model with C = 0.0001 and solver = liblinear\n",
      "Training model with C = 0.0001 and solver = sag\n",
      "Training model with C = 0.0001 and solver = saga\n",
      "Training model with C = 0.001 and solver = newton-cg\n",
      "Training model with C = 0.001 and solver = lbfgs\n",
      "Training model with C = 0.001 and solver = liblinear\n",
      "Training model with C = 0.001 and solver = sag\n",
      "Training model with C = 0.001 and solver = saga\n",
      "Training model with C = 0.01 and solver = newton-cg\n",
      "Training model with C = 0.01 and solver = lbfgs\n",
      "Training model with C = 0.01 and solver = liblinear\n",
      "Training model with C = 0.01 and solver = sag\n",
      "Training model with C = 0.01 and solver = saga\n",
      "Training model with C = 0.1 and solver = newton-cg\n",
      "Training model with C = 0.1 and solver = lbfgs\n",
      "Training model with C = 0.1 and solver = liblinear\n",
      "Training model with C = 0.1 and solver = sag\n",
      "Training model with C = 0.1 and solver = saga\n",
      "Training model with C = 1 and solver = newton-cg\n",
      "Training model with C = 1 and solver = lbfgs\n",
      "Training model with C = 1 and solver = liblinear\n",
      "Training model with C = 1 and solver = sag\n",
      "Training model with C = 1 and solver = saga\n",
      "               C     Solver  Mean accuracy  Mean EOD\n",
      "0   1.000000e-08  newton-cg       0.526081  0.000000\n",
      "1   1.000000e-08      lbfgs       0.526081  0.000000\n",
      "2   1.000000e-08  liblinear       0.727362  0.873509\n",
      "3   1.000000e-08        sag       0.526081  0.000000\n",
      "4   1.000000e-08       saga       0.526081  0.000000\n",
      "5   1.000000e-07  newton-cg       0.526081  0.000000\n",
      "6   1.000000e-07      lbfgs       0.526081  0.000000\n",
      "7   1.000000e-07  liblinear       0.727447  0.873648\n",
      "8   1.000000e-07        sag       0.526081  0.000000\n",
      "9   1.000000e-07       saga       0.526081  0.000000\n",
      "10  1.000000e-06  newton-cg       0.542484  0.019263\n",
      "11  1.000000e-06      lbfgs       0.542262  0.019263\n",
      "12  1.000000e-06  liblinear       0.728011  0.872606\n",
      "13  1.000000e-06        sag       0.542894  0.020327\n",
      "14  1.000000e-06       saga       0.542390  0.019263\n",
      "15  1.000000e-05  newton-cg       0.693959  0.891340\n",
      "16  1.000000e-05      lbfgs       0.693899  0.891110\n",
      "17  1.000000e-05  liblinear       0.732750  0.866173\n",
      "18  1.000000e-05        sag       0.694215  0.891219\n",
      "19  1.000000e-05       saga       0.693950  0.891075\n",
      "20  1.000000e-04  newton-cg       0.742663  0.787601\n",
      "21  1.000000e-04      lbfgs       0.742689  0.787654\n",
      "22  1.000000e-04  liblinear       0.743628  0.785420\n",
      "23  1.000000e-04        sag       0.742689  0.787515\n",
      "24  1.000000e-04       saga       0.742638  0.787601\n",
      "25  1.000000e-03  newton-cg       0.751022  0.652050\n",
      "26  1.000000e-03      lbfgs       0.750988  0.652286\n",
      "27  1.000000e-03  liblinear       0.751202  0.652404\n",
      "28  1.000000e-03        sag       0.751022  0.652101\n",
      "29  1.000000e-03       saga       0.751022  0.652320\n",
      "30  1.000000e-02  newton-cg       0.751202  0.615131\n",
      "31  1.000000e-02      lbfgs       0.751253  0.615774\n",
      "32  1.000000e-02  liblinear       0.751202  0.615027\n",
      "33  1.000000e-02        sag       0.751185  0.615148\n",
      "34  1.000000e-02       saga       0.751193  0.615113\n",
      "35  1.000000e-01  newton-cg       0.750903  0.611918\n",
      "36  1.000000e-01      lbfgs       0.750869  0.612218\n",
      "37  1.000000e-01  liblinear       0.750886  0.611901\n",
      "38  1.000000e-01        sag       0.750877  0.611901\n",
      "39  1.000000e-01       saga       0.750877  0.611901\n",
      "40  1.000000e+00  newton-cg       0.750852  0.611498\n",
      "41  1.000000e+00      lbfgs       0.750860  0.611850\n",
      "42  1.000000e+00  liblinear       0.750860  0.611550\n",
      "43  1.000000e+00        sag       0.750826  0.611498\n",
      "44  1.000000e+00       saga       0.750869  0.611567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into train-val and test sets\n",
    "train_and_val_data, test_data = data_for_aif.split([0.7], shuffle=True, seed=0)\n",
    "\n",
    "# Set up loop for hyperparameter tuning/exploration (C, regularisation strength)\n",
    "results = pd.DataFrame(columns=['C', 'Solver','Mean accuracy', 'Mean EOD'])\n",
    "\n",
    "for C in [ 10**i for i in range(-8, 1, 1)]: # loop through different C values\n",
    "    for solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']: # loop through different solvers\n",
    "        print(\"Training model with C =\", C, \"and solver =\", solver)\n",
    "        \n",
    "        # Initialise model\n",
    "        model = LogisticRegression(solver=solver, C=C, random_state=0)\n",
    "\n",
    "        # Set up loop for different train-val splits\n",
    "        accuracy_list = [] # initialize list to store accuracy values\n",
    "        eod_list = [] # initialize list to store EOD values\n",
    "\n",
    "        for seed_index in [10*i for i in range(5)]:\n",
    "            # Split train-val set into train and val sets\n",
    "            train_data, val_data = train_and_val_data.split([0.8], shuffle=True, seed=seed_index)\n",
    "\n",
    "            # Normalize the train and val datasets\n",
    "            scale_orig = StandardScaler()\n",
    "            x_train = scale_orig.fit_transform(train_data.features)\n",
    "            y_train = train_data.labels.ravel()\n",
    "            x_val = scale_orig.transform(val_data.features)\n",
    "            y_val = val_data.labels.ravel()\n",
    "\n",
    "            # Model training and prediction\n",
    "            model.fit(x_train,y_train)\n",
    "            predictions = model.predict(x_val)\n",
    "            val_pred = val_data.copy()\n",
    "            val_pred.labels = predictions\n",
    "\n",
    "            # Metrics\n",
    "            metrics = ClassificationMetric(val_data, val_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "            eod = (metrics.equal_opportunity_difference()) # Equal opportunity difference\n",
    "            \n",
    "            # Store accuracy and EOD values\n",
    "            accuracy_list.append(sum(predictions==y_val)/len(y_val))\n",
    "            eod_list.append(eod)\n",
    "        \n",
    "        # Calculate mean accuracy and EOD values and append to lists\n",
    "        mean_accuracy = np.mean(accuracy_list)\n",
    "        mean_eod = np.mean(eod_list)\n",
    "        \n",
    "        # Append results to dataframe\n",
    "        new_result  = pd.DataFrame([[C, solver, mean_accuracy, mean_eod]], columns=['C', 'Solver','Mean accuracy', 'Mean EOD'])\n",
    "        results = pd.concat([results, new_result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:\n",
      "        C Solver  Mean accuracy  Mean EOD\n",
      "31  0.01  lbfgs       0.751253  0.615774 \n",
      "\n",
      "Best EOD:\n",
      "               C     Solver  Mean accuracy  Mean EOD\n",
      "0  1.000000e-08  newton-cg       0.526081       0.0\n",
      "1  1.000000e-08      lbfgs       0.526081       0.0\n",
      "3  1.000000e-08        sag       0.526081       0.0\n",
      "4  1.000000e-08       saga       0.526081       0.0\n",
      "5  1.000000e-07  newton-cg       0.526081       0.0\n",
      "6  1.000000e-07      lbfgs       0.526081       0.0\n",
      "8  1.000000e-07        sag       0.526081       0.0\n",
      "9  1.000000e-07       saga       0.526081       0.0 \n",
      "\n",
      "Lowest non-zero EOD:\n",
      "            C     Solver  Mean accuracy  Mean EOD\n",
      "10  0.000001  newton-cg       0.542484  0.019263\n",
      "11  0.000001      lbfgs       0.542262  0.019263\n",
      "14  0.000001       saga       0.542390  0.019263 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find highest accuracy and lowest EOD\n",
    "highest_accuracy = results['Mean accuracy'].max()\n",
    "lowest_eod = results['Mean EOD'].min() \n",
    "\n",
    "# find lowest EOD that is not 0\n",
    "lowest_nonzero_eod = results.loc[results['Mean EOD'] != 0]['Mean EOD'].min()\n",
    "\n",
    "# find the corresponding C and solver values\n",
    "best_accuracy = results.loc[results['Mean accuracy'] == highest_accuracy]\n",
    "best_eod = results.loc[results['Mean EOD'] == lowest_eod]\n",
    "best_nonzero_eod = results.loc[results['Mean EOD'] == lowest_nonzero_eod]\n",
    "\n",
    "print(f\"Best accuracy:\\n\", best_accuracy, f'\\n')\n",
    "print(f\"Best EOD:\\n\", best_eod, f'\\n')\n",
    "print(f\"Lowest non-zero EOD:\\n\", best_nonzero_eod, f'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with C = 1e-08 and solver = newton-cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/n0qzq7z56gj9ts7dnzkhpbr80000gn/T/ipykernel_7995/2966597457.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with C = 1e-08 and solver = lbfgs\n",
      "Training model with C = 1e-08 and solver = liblinear\n",
      "Training model with C = 1e-08 and solver = sag\n",
      "Training model with C = 1e-08 and solver = saga\n",
      "Training model with C = 1e-07 and solver = newton-cg\n",
      "Training model with C = 1e-07 and solver = lbfgs\n",
      "Training model with C = 1e-07 and solver = liblinear\n",
      "Training model with C = 1e-07 and solver = sag\n",
      "Training model with C = 1e-07 and solver = saga\n",
      "Training model with C = 1e-06 and solver = newton-cg\n",
      "Training model with C = 1e-06 and solver = lbfgs\n",
      "Training model with C = 1e-06 and solver = liblinear\n",
      "Training model with C = 1e-06 and solver = sag\n",
      "Training model with C = 1e-06 and solver = saga\n",
      "Training model with C = 1e-05 and solver = newton-cg\n",
      "Training model with C = 1e-05 and solver = lbfgs\n",
      "Training model with C = 1e-05 and solver = liblinear\n",
      "Training model with C = 1e-05 and solver = sag\n",
      "Training model with C = 1e-05 and solver = saga\n",
      "Training model with C = 0.0001 and solver = newton-cg\n",
      "Training model with C = 0.0001 and solver = lbfgs\n",
      "Training model with C = 0.0001 and solver = liblinear\n",
      "Training model with C = 0.0001 and solver = sag\n",
      "Training model with C = 0.0001 and solver = saga\n",
      "Training model with C = 0.001 and solver = newton-cg\n",
      "Training model with C = 0.001 and solver = lbfgs\n",
      "Training model with C = 0.001 and solver = liblinear\n",
      "Training model with C = 0.001 and solver = sag\n",
      "Training model with C = 0.001 and solver = saga\n",
      "Training model with C = 0.01 and solver = newton-cg\n",
      "Training model with C = 0.01 and solver = lbfgs\n",
      "Training model with C = 0.01 and solver = liblinear\n",
      "Training model with C = 0.01 and solver = sag\n",
      "Training model with C = 0.01 and solver = saga\n",
      "Training model with C = 0.1 and solver = newton-cg\n",
      "Training model with C = 0.1 and solver = lbfgs\n",
      "Training model with C = 0.1 and solver = liblinear\n",
      "Training model with C = 0.1 and solver = sag\n",
      "Training model with C = 0.1 and solver = saga\n",
      "Training model with C = 1 and solver = newton-cg\n",
      "Training model with C = 1 and solver = lbfgs\n",
      "Training model with C = 1 and solver = liblinear\n",
      "Training model with C = 1 and solver = sag\n",
      "Training model with C = 1 and solver = saga\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into train-val and test sets\n",
    "train_and_val_data, test_data = data_for_aif.split([0.7], shuffle=True, seed=0)\n",
    "\n",
    "# Set up loop for hyperparameter tuning/exploration (C, regularisation strength)\n",
    "results = pd.DataFrame(columns=['C', 'Solver','Mean accuracy', 'Mean EOD'])\n",
    "\n",
    "for C in [ 10**i for i in range(-8, 1, 1)]: # loop through different C values\n",
    "    for solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']: # loop through different solvers\n",
    "        print(\"Training model with C =\", C, \"and solver =\", solver)\n",
    "        \n",
    "        # Initialise model\n",
    "        model = LogisticRegression(solver=solver, C=C, random_state=0)\n",
    "\n",
    "        # Set up loop for different train-val splits\n",
    "        accuracy_list = [] # initialize list to store accuracy values\n",
    "        eod_list = [] # initialize list to store EOD values\n",
    "\n",
    "        for seed_index in [10*i for i in range(5)]:\n",
    "            # Split train-val set into train and val sets\n",
    "            train_data, val_data = train_and_val_data.split([0.8], shuffle=True, seed=seed_index)\n",
    "\n",
    "            # Transform the original dataset via reweighing\n",
    "            RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "            train_data = RW.fit_transform(train_data)\n",
    "            \n",
    "            # Normalize the train and val datasets\n",
    "            scale_orig = StandardScaler()\n",
    "            x_train = scale_orig.fit_transform(train_data.features)\n",
    "            y_train = train_data.labels.ravel()\n",
    "            x_val = scale_orig.transform(val_data.features)\n",
    "            y_val = val_data.labels.ravel()\n",
    "            \n",
    "            # Model training and prediction\n",
    "            model.fit(x_train,y_train)\n",
    "            predictions = model.predict(x_val)\n",
    "            val_pred = val_data.copy()\n",
    "            val_pred.labels = predictions\n",
    "\n",
    "            # Metrics\n",
    "            metrics = ClassificationMetric(val_data, val_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "            eod = (metrics.equal_opportunity_difference()) # Equal opportunity difference\n",
    "            \n",
    "            # Store accuracy and EOD values\n",
    "            accuracy_list.append(sum(predictions==y_val)/len(y_val))\n",
    "            eod_list.append(eod)\n",
    "        \n",
    "        # Calculate mean accuracy and EOD values and append to lists\n",
    "        mean_accuracy = np.mean(accuracy_list)\n",
    "        mean_eod = np.mean(eod_list)\n",
    "        \n",
    "        # Append results to dataframe\n",
    "        new_result  = pd.DataFrame([[C, solver, mean_accuracy, mean_eod]], columns=['C', 'Solver','Mean accuracy', 'Mean EOD'])\n",
    "        results = pd.concat([results, new_result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               C     Solver  Mean accuracy  Mean EOD\n",
      "0   1.000000e-08  newton-cg       0.526081  0.000000\n",
      "1   1.000000e-08      lbfgs       0.526081  0.000000\n",
      "2   1.000000e-08  liblinear       0.727362  0.873509\n",
      "3   1.000000e-08        sag       0.526081  0.000000\n",
      "4   1.000000e-08       saga       0.526081  0.000000\n",
      "5   1.000000e-07  newton-cg       0.526081  0.000000\n",
      "6   1.000000e-07      lbfgs       0.526081  0.000000\n",
      "7   1.000000e-07  liblinear       0.727447  0.873648\n",
      "8   1.000000e-07        sag       0.526081  0.000000\n",
      "9   1.000000e-07       saga       0.526081  0.000000\n",
      "10  1.000000e-06  newton-cg       0.542484  0.019263\n",
      "11  1.000000e-06      lbfgs       0.542262  0.019263\n",
      "12  1.000000e-06  liblinear       0.728011  0.872606\n",
      "13  1.000000e-06        sag       0.542894  0.020327\n",
      "14  1.000000e-06       saga       0.542390  0.019263\n",
      "15  1.000000e-05  newton-cg       0.693959  0.891340\n",
      "16  1.000000e-05      lbfgs       0.693899  0.891110\n",
      "17  1.000000e-05  liblinear       0.732750  0.866173\n",
      "18  1.000000e-05        sag       0.694215  0.891219\n",
      "19  1.000000e-05       saga       0.693950  0.891075\n",
      "20  1.000000e-04  newton-cg       0.742663  0.787601\n",
      "21  1.000000e-04      lbfgs       0.742689  0.787654\n",
      "22  1.000000e-04  liblinear       0.743628  0.785420\n",
      "23  1.000000e-04        sag       0.742689  0.787515\n",
      "24  1.000000e-04       saga       0.742638  0.787601\n",
      "25  1.000000e-03  newton-cg       0.751022  0.652050\n",
      "26  1.000000e-03      lbfgs       0.750988  0.652286\n",
      "27  1.000000e-03  liblinear       0.751202  0.652404\n",
      "28  1.000000e-03        sag       0.751022  0.652101\n",
      "29  1.000000e-03       saga       0.751022  0.652320\n",
      "30  1.000000e-02  newton-cg       0.751202  0.615131\n",
      "31  1.000000e-02      lbfgs       0.751253  0.615774\n",
      "32  1.000000e-02  liblinear       0.751202  0.615027\n",
      "33  1.000000e-02        sag       0.751185  0.615148\n",
      "34  1.000000e-02       saga       0.751193  0.615113\n",
      "35  1.000000e-01  newton-cg       0.750903  0.611918\n",
      "36  1.000000e-01      lbfgs       0.750869  0.612218\n",
      "37  1.000000e-01  liblinear       0.750886  0.611901\n",
      "38  1.000000e-01        sag       0.750877  0.611901\n",
      "39  1.000000e-01       saga       0.750877  0.611901\n",
      "40  1.000000e+00  newton-cg       0.750852  0.611498\n",
      "41  1.000000e+00      lbfgs       0.750860  0.611850\n",
      "42  1.000000e+00  liblinear       0.750860  0.611550\n",
      "43  1.000000e+00        sag       0.750826  0.611498\n",
      "44  1.000000e+00       saga       0.750869  0.611567\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
